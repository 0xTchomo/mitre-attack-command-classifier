# MITRE ATT&CK Command Classification using ML & DL

> **From raw command-line telemetry to actionable ATT&CK technique classification**

This project explores how **machine learning (ML)** and **deep learning (DL)** models can be used to automatically classify **command-line process titles (`proctitle`)** into **MITRE ATT&CK techniques**, a key challenge in modern **host-based threat detection**.

The work combines **careful data analysis**, **feature engineering tailored to cybersecurity artifacts**, and **model comparison**, bridging the gap between academic experimentation and realistic defensive use cases.

---

## ğŸ¯ Project Motivation

Command-line executions are one of the most informative traces of adversarial behavior on a system.  
However, raw commands are:

- highly variable,
- context-dependent,
- noisy (paths, flags, IPs, URLs, credentials),
- and often reused across multiple attack techniques.

This project investigates:

- **Can we automatically map commands to MITRE ATT&CK techniques?**
- **How far can classical ML go with the right feature engineering?**
- **What does deep learning add when modeling command semantics?**

---

## ğŸ§  Dataset & Threat Model

### Dataset origin
The dataset comes from the **Casino Limit CTF scenario**, where multiple attacking teams were monitored during a realistic attack simulation.

Each sample represents:
- a **command-line process title (`proctitle`)**
- a **MITRE ATT&CK technique label**
- an associated **execution count** (frequency in the original logs)

> ğŸ“Œ The raw dataset is not generated synthetically: it reflects real attacker behavior observed during a competitive offensive scenario.

### Label consolidation
- Initial labels: **67 ATT&CK techniques**
- Final labels: **37 techniques**
- Rare techniques are grouped into a dedicated class:  
  **`Z999: Other Low-Frequency Techniques`**

This avoids extreme class imbalance while preserving semantic meaning.

---

## ğŸ§ª Methodology Overview

The project follows a **progressive, reproducible pipeline**:

### 1ï¸âƒ£ Exploratory Data Analysis (EDA)
Notebook: `notebooks/01_eda.ipynb`

Key analyses:
- distribution of techniques vs grouped techniques,
- difference between **unique commands** and **real execution frequency** (`count`),
- explanation of artificial duplicates created by technique grouping,
- consolidation of `(proctitle, technique_grouped)` pairs by summing counts.

**Key insight**  
> A command can legitimately map to multiple attack techniques depending on context â€” grouping and weighting are not data bugs, but threat modeling decisions.

---

### 2ï¸âƒ£ Machine Learning Baseline (TF-IDF + Logistic Regression)
Notebook: `notebooks/02_ml_lr.ipynb`

#### Feature representation
- **TF-IDF** with:
  - unigrams + bigrams,
  - maximum vocabulary size = 5,000.

#### âš ï¸ Cybersecurity-aware tokenization (key contribution)
A **custom token pattern** is introduced to preserve:
- IP addresses (`10.35.108.10:22`)
- URLs (`http://malicious.com/payload.sh`)
- file paths (`/etc/passwd`)
- command flags (`--color=auto`)
- environment variables (`$HOME`, `${PATH}`)

This significantly improves semantic signal compared to default NLP tokenization.

#### Training strategy
- Stratified **60 / 20 / 20** split (train / validation / test)
- **Sample weights = execution count**
- Weighted metrics (`accuracy_weighted`, `f1_weighted`)
- Hyperparameter tuning with `GridSearchCV`

**Result**  
Classical ML, when combined with domain-specific feature engineering, provides a **strong and interpretable baseline**.

---

### 3ï¸âƒ£ Deep Learning Baseline (LSTM)
Notebook: `notebooks/03_dl_lstm.ipynb`

The DL approach models commands as **sequences**, rather than bags of tokens.

Pipeline:
- Tokenization with OOV handling
- Fixed-length padding
- Embedding layer
- LSTM for sequential modeling
- Softmax output over 37 ATT&CK classes

Why LSTM?
- captures ordering and structure (`command â†’ flags â†’ arguments â†’ paths`)
- learns implicit patterns missed by linear models

This model serves as a **semantic baseline**, not a replacement for ML.

---

## ğŸ“Š Key Findings

- **Execution frequency matters**: ignoring the `count` column severely misrepresents attacker behavior.
- **Grouping rare techniques is necessary**, but must be explained and handled carefully.
- **Custom tokenization is critical** in cybersecurity NLP.
- **ML models remain highly competitive** when domain knowledge is injected.
- **DL models add flexibility**, but at higher computational and interpretability cost.

---

## ğŸ“ Project Structure

mitre-attack-command-classifier/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda.ipynb # Dataset exploration and consolidation
â”‚ â”œâ”€â”€ 02_ml_lr.ipynb # TF-IDF + Logistic Regression baseline
â”‚ â””â”€â”€ 03_dl_lstm.ipynb # LSTM deep learning model
â”‚
â”œâ”€â”€ command_classifier/
â”‚ â”œâ”€â”€ train_lr.py # ML training pipeline
â”‚ â”œâ”€â”€ train_lstm.py # DL training pipeline
â”‚ â”œâ”€â”€ predict.py # Inference on new commands
â”‚ â”œâ”€â”€ common.py # Shared utilities
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Full dataset (see data/README.md)
â”‚ â”œâ”€â”€ sample/ # Lightweight sample CSV
â”‚ â””â”€â”€ README.md # Dataset access instructions
â”‚
â”œâ”€â”€ requirements.txt # ML dependencies
â”œâ”€â”€ requirements-dl.txt # DL dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md


---

## ğŸš€ Getting Started

### Installation

```bash
git clone https://github.com/0xTchomo/mitre-attack-command-classifier.git
cd mitre-attack-command-classifier
pip install -r requirements.txt

